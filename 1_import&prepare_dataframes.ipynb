{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 80)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import re \n",
    "\n",
    "from googleplaces import GooglePlaces, types, lang\n",
    "import googlemaps\n",
    "\n",
    "import urllib.request as urllib2\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Colorcode for Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_colors = {'just green':'#008000','dark green':'#004C00', 'mid green':'#669366', 'light green':'#b2c9b2', \n",
    "               'just red':'#FF0000','dark red':'#cc0000', 'mid red':'#c16666', 'light red':'#e0b2b2', \n",
    "               'just blue':'#0000FF','dark blue':'#000066', 'mid blue':'#6666c1', 'light blue':'#b2b2e0', \n",
    "               'just yellow':'#FFFF00','dark yellow':'#999900', 'mid yellow':'#c1c166', 'light yellow':'#e0e0b2', \n",
    "               'just cyan':'#00FFFF','dark cyan':'#009999', 'mid cyan':'#66c1c1', 'light cyan':'#b2e0e0', \n",
    "               'just magenta':'#FF00FF', 'dark magenta':'#990099', 'mid magenta':'#c166c1', 'light magenta':'#e0b2e0',\n",
    "               'just grey':'#808080','dark grey':'#595959', 'mid grey':'#999999', 'light grey':'#D8D8D8', \\\n",
    "              'super light grey':'#FDFDFD'}\n",
    "df_colors = pd.DataFrame.from_dict(dict_colors, orient='index')\n",
    "df_colors.to_csv('.//for_paper//colors//df_colors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GREEN\n",
    "list_colors_green = ['#d1e7d1','#7fd97f', '#7fe67f', '#99eb99','#328f32','#198c19','#32c232',\n",
    "                     '#004C00','#669366', '#eff7ef','#4c704c','#b2c9b2','#008000',\n",
    "                     '#668466','#66b266','#00b300', '#3d6a3d','#e0efe0','#a3d0a3',\n",
    "                     '#198119','#00a100', '#c1e0c1','#4c9d4c','#329932','#4ca64c',\n",
    "                     '#007300','#006600', '#7f997f','#99ad99',\n",
    "                     '#32d732','#b2d8b2','#005900','#66e166', \n",
    "                     '#5ba05b','#518e51', '#93c993',\n",
    "                     '#325b32','#477c47','#75b975',]\n",
    "list_colors_green = set(list_colors_green)\n",
    "dict_colors_green = dict(zip(list(range(len(list_colors_green))),list_colors_green))\n",
    "df_colors_green = pd.DataFrame.from_dict(dict_colors_green, orient='index')\n",
    "df_colors_green.to_csv('.//for_paper//colors//df_colors_green.csv')\n",
    "\n",
    "list_colors_light_green = ['#e0efe0', '#b2d8b2', '#7fe67f', '#eff7ef', \\\n",
    "                           '#c1e0c1', '#99eb99', '#b2c9b2']\n",
    "\n",
    "list_colors_light_green = set(list_colors_light_green)\n",
    "dict_colors_light_green = dict(zip(list(range(len(list_colors_light_green))),list_colors_light_green))\n",
    "df_colors_light_green = pd.DataFrame.from_dict(dict_colors_light_green, orient='index')\n",
    "df_colors_light_green.to_csv('.//for_paper//colors//df_colors_light_green.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLUE\n",
    "list_colors_blue = ['#b2b2e0', '#000066', '#4c4cff', '#CBCBFF', '#b2b2ff', '#0000b2', \n",
    "                    '#4c4c93', '#9999c1', '#cccce0','#00007f']\n",
    "\n",
    "list_colors_blue = set(list_colors_blue)\n",
    "dict_colors_blue = dict(zip(list(range(len(list_colors_blue))),list_colors_blue))\n",
    "df_colors_blue = pd.DataFrame.from_dict(dict_colors_blue, orient='index')\n",
    "df_colors_blue.to_csv('.//for_paper//colors//df_colors_blue.csv')\n",
    "\n",
    "list_colors_light_blue = ['#b2b2e0', '#1A8DFF','#b2b2ff', '#B3B3FF',\\\n",
    "                          '#CBCBFF','#CCCCFF', '#eaeaff', '#e0e0ff', '#e8e8ff']\n",
    "\n",
    "list_colors_light_blue = set(list_colors_light_blue)\n",
    "dict_colors_light_blue = dict(zip(list(range(len(list_colors_light_blue))),list_colors_light_blue))\n",
    "df_colors_light_blue = pd.DataFrame.from_dict(dict_colors_light_blue, orient='index')\n",
    "df_colors_light_blue.to_csv('.//for_paper//colors//df_colors_light_blue.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RED\n",
    "list_colors_red = ['#ff0000','#ff4c4c','#ff3232','#bf7f7f','#b20000',\n",
    "                   '#990000','#c16666', '#cc6666','#e0b2b2','#ff1919','#ff2f2f',\n",
    "                   '#934c4c','#b25959','#b27f7f','#c19999',\n",
    "                   '#ff6666','#cc0000','#ffb2b2','#ff8c8c','#ffcccc','#ffe5e5',\n",
    "                  '#e50000','#a36666','#ffbfbf','#990000','#a54c4c',\n",
    "                  '#e55b5b','#cc5151','#ff1919','#b24747','#ff7f7f','#993d3d',\n",
    "                   '#b26666','#cb9999','#d8b2b2', '#b30000','#a10000',\n",
    "                   '#e57373','#cc6666','#b25959','#ff9999','#ffa6a6','#ffb2b2','#ffcccc',\n",
    "                  '#e57373','#994c4c'] #'#ff4646',\n",
    "list_colors_red = set(list_colors_red)\n",
    "dict_colors_red = dict(zip(list(range(len(list_colors_red))),list_colors_red))\n",
    "df_colors_red = pd.DataFrame.from_dict(dict_colors_red, orient='index')\n",
    "df_colors_red.to_csv('.//for_paper//colors//df_colors_red.csv')\n",
    "\n",
    "list_colors_light_red = ['#ffe5e5','ffb2b2', '#cb9999',\\\n",
    "                         '#ffcccc', '#d8b2b2', '#ffa6a6', '#ffbfbf',   '#ff6666']\n",
    "list_colors_light_red = set(list_colors_light_red)\n",
    "dict_colors_light_red = dict(zip(list(range(len(list_colors_light_red))),list_colors_light_red))\n",
    "df_colors_light_red = pd.DataFrame.from_dict(dict_colors_light_red, orient='index')\n",
    "df_colors_light_red.to_csv('.//for_paper//colors//df_colors_light_red.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import raw dataframes & clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = '2020_01'# '2019_01', '2018a_07' ##for new dataframes just insert the year&month here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './/'\n",
    "filename = 'ds_Coalswarm_Plant_Tracker_'+z+'.csv'\n",
    "df_dev_pltr = pd.read_csv(folder+filename ,sep=';', encoding='latin-1', decimal=',', thousands='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean data:\n",
    "\n",
    "#sometimes values were not calculated:\n",
    "df_dev_pltr.replace({'#WERT!':'0'}, inplace=True)\n",
    "\n",
    "#some values are nan:\n",
    "df_dev_pltr.fillna(0, inplace=True)\n",
    "\n",
    "#some years stated as strings:\n",
    "dict_year_names = {'13th plan':0, 'Unknown':0, \"1960's\":1960, '1960s':1960, \\\n",
    "\"1970's\":1970, '1970\\x91 s':1970, \"1980's\":1980, '1980s':1980, '1980\\x91s':1980, \\\n",
    "\"1990's\":1990,'2021/2022':2021, '2025/2026':2025,'2027/2028':2027,'2028-2030':2029, \\\n",
    "'2028/2029':2028, 'after 2027':2028,  '2030s':2030, '2033-2035':2034, np.nan:0, 'Unclear':0, \\\n",
    "'no activity since 2010':0, 'After 2030':2030}\n",
    "for x in df_dev_pltr.index:\n",
    "    var_year_name = df_dev_pltr.loc[x,'Year']\n",
    "    if var_year_name in dict_year_names.keys():\n",
    "        df_dev_pltr.loc[x,'Year'] = dict_year_names[var_year_name]\n",
    "\n",
    "#rename columns, name changes from 2019 to 2020!\n",
    "dict_column_names = {'Lifetime CO2 (40 years)':'Lifetime CO2', 'Lifetime CO2 (million tonnes)':'Lifetime CO2', \\\n",
    "                     'Country':'Site Country', 'Parent':'DEV'} \n",
    "for x in df_dev_pltr.columns:\n",
    "    if x in dict_column_names.keys():\n",
    "        df_dev_pltr.rename(columns={x: dict_column_names[x]}, inplace=True)\n",
    "\n",
    "#some capacity are string -> insert 0 and turn whole column into int\n",
    "#if 2.000 can't convert to int -->remove '.' first\n",
    "for x in df_dev_pltr.index:\n",
    "    if  type(df_dev_pltr.loc[x, 'Capacity (MW)']) == str:\n",
    "        df_dev_pltr.loc[x, 'Capacity (MW)'] = df_dev_pltr.loc[x, 'Capacity (MW)'].replace(\".\", \"\")\n",
    "    if df_dev_pltr.loc[x, 'Capacity (MW)'] in ['TBD', 'unknown', 'Unknown']:\n",
    "        df_dev_pltr.loc[x, 'Capacity (MW)'] = 0\n",
    "df_dev_pltr['Capacity (MW)'] = df_dev_pltr['Capacity (MW)'].astype(int)\n",
    "#some Capacity (MW) are empty -> insert 0\n",
    "for x in df_dev_pltr.index:\n",
    "    if np.isnan(df_dev_pltr.loc[x, 'Capacity (MW)']):\n",
    "        df_dev_pltr.loc[x, 'Capacity (MW)'] = 0\n",
    "\n",
    "#turn all columns into int (if they contained str when reading the csv they are str)\n",
    "#have to replace ',' with '.' first\n",
    "list_columns_str = ['Lifetime CO2', 'Annual CO2 (million tonnes / annum)']\n",
    "for x in df_dev_pltr.columns:\n",
    "    if x in list_columns_str:\n",
    "        if df_dev_pltr.loc[5, x] == str: #only do, when they are str!\n",
    "            df_dev_pltr[x] = df_dev_pltr[x].str.replace(',','.')\n",
    "            df_dev_pltr[x] = df_dev_pltr[x].astype(float)     \n",
    "\n",
    "#for 2020_01 some status are lower some upper case -->turn all into upper!\n",
    "for x in df_dev_pltr.index:\n",
    "     df_dev_pltr.loc[x, 'Status'] = df_dev_pltr.loc[x, 'Status'].title()\n",
    "\n",
    "#sometimes DEV = 0 P:'int' object has no attribute 'split' S:change to unknown\n",
    "for x in df_dev_pltr.index:\n",
    "     if df_dev_pltr.loc[x, 'DEV'] == 0:\n",
    "        df_dev_pltr.loc[x, 'DEV'] = 'unknown'        \n",
    "\n",
    "#save as csv\n",
    "df_dev_unsplit = df_dev_pltr.copy()\n",
    "df_dev_unsplit.to_csv('.//for_paper//df_overview//df_dev_unsplit_'+z+'.csv', decimal=',', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00%"
     ]
    }
   ],
   "source": [
    "#split dataframe:\n",
    "#P: dataset often has several sponsors for one unit\n",
    "#S-> split units by sponsor name and split capacity respectively\n",
    "\n",
    "df_dev_split = pd.DataFrame(columns = df_dev_unsplit.columns)\n",
    "\n",
    "l = len(df_dev_unsplit.index)\n",
    "\n",
    "for row_number in range(l):\n",
    "    therow = df_dev_unsplit.loc[row_number].copy()\n",
    "    list_split = df_dev_unsplit.loc[row_number, \"DEV\"].split(\",\")\n",
    "    splitlength = len(list_split)\n",
    "    if splitlength > 1:\n",
    "        var_capacity = df_dev_unsplit.loc[row_number, 'Capacity (MW)']\n",
    "        var_ann_co2 = df_dev_unsplit.loc[row_number, 'Annual CO2 (million tonnes / annum)']\n",
    "        var_life_co2 = df_dev_unsplit.loc[row_number, 'Lifetime CO2']\n",
    "\n",
    "        for s in list_split:\n",
    "            new_row = therow\n",
    "            new_row[\"DEV\"] = s\n",
    "            new_row['Capacity (MW)'] = int(var_capacity/len(list_split))\n",
    "            new_row['Annual CO2 (million tonnes / annum)'] = int(var_ann_co2/len(list_split))\n",
    "            new_row['Lifetime CO2'] = int(var_life_co2/len(list_split))\n",
    "\n",
    "            df_dev_split = df_dev_split.append(new_row,ignore_index=True)\n",
    "    else:\n",
    "        df_dev_split = df_dev_split.append(therow,ignore_index=True)\n",
    "\n",
    "    print('\\r'+ '%.2f' % (row_number/(l/100)+0.01) + '%', end='', flush=True)\n",
    "\n",
    "\n",
    "#many annoying characters in the DEVs after splitting, remove all!\n",
    "df_dev_split['DEV'] = list(map(lambda x: re.sub(r\"\\((\\d+)%\\)\", '', x), df_dev_split['DEV']))\n",
    "df_dev_split['DEV'] = list(map(lambda x: re.sub(r\"\\((\\d+).(\\d+)%\\)\", '', x), df_dev_split['DEV']))\n",
    "df_dev_split['DEV'] = list(map(lambda x: re.sub(r\"(\\d+).(\\d+)%\", '', x), df_dev_split['DEV']))\n",
    "df_dev_split['DEV'] = list(map(lambda x: re.sub(r\"(\\d+)%\\)\", '', x), df_dev_split['DEV']))\n",
    "df_dev_split['DEV'] = list(map(lambda x: re.sub(r\"(\\d+) %\\)\", '', x), df_dev_split['DEV']))\n",
    "df_dev_split['DEV'] = list(map(lambda x: re.sub(r\"(\\d+)%\", '', x), df_dev_split['DEV']))\n",
    "df_dev_split['DEV'] = list(map(lambda x: re.sub(r\"\\((\\d+)\", '', x), df_dev_split['DEV']))\n",
    "df_dev_split['DEV'] = list(map(str.strip, df_dev_split['DEV']))\n",
    "\n",
    "#if dev is nan, insert 'unknown'\n",
    "df_dev_split['DEV'].replace(to_replace=np.nan, value='unknown', inplace=True)\n",
    "\n",
    "list_change = ['','??????????????', 'Unknown', 'TBD', 'Other']\n",
    "for x in df_dev_split.index:\n",
    "    if df_dev_split.loc[x, 'DEV'] in list_change:\n",
    "        df_dev_split.loc[x, 'DEV'] = 'unknown'\n",
    "\n",
    "#export as csv\n",
    "df_dev_split.to_csv('.//for_paper//df_overview//df_dev_split_'+z+'.csv', decimal=',', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "for 2020_01:\n",
      "annual CO2 split = 18924.12\n",
      "Lifet CO2 split = 608540.3200000001\n",
      "Capa split = 4684969\n",
      "Construction split = 199543\n",
      "len split = 16693\n",
      "\n",
      "annual CO2 unsplit = 21995.23\n",
      "Lifet CO2 unsplit = 611558.95\n",
      "Capa unsplit = 4685840\n",
      "Construction unsplit = 199572\n",
      "len unsplit = 12875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check, whether sum stayed the same!\n",
    "dict_df_dev_unsplit_and_split = {'unsplit':df_dev_unsplit,'split':df_dev_split}\n",
    "print('\\n'+'for '+z +':')\n",
    "for u in ['split', 'unsplit']:\n",
    "    df_a = dict_df_dev_unsplit_and_split[u]\n",
    "    print('annual CO2 '+u+' = '+str(df_a['Annual CO2 (million tonnes / annum)'].sum()))\n",
    "    print('Lifet CO2 '+u+' = '+str(df_a['Lifetime CO2'].sum()))\n",
    "    print('Capa '+u+' = '+str(df_a['Capacity (MW)'].sum()))\n",
    "    print('Construction '+u+' = '+ str(df_a[df_a['Status']=='Construction']['Capacity (MW)'].sum()))\n",
    "    print('len '+u+' = '+str(len(df_a))+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loans & underwriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two datasets available, from 2017 and 2019. Import both and merge them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grand Total is 907793.7799999999\n"
     ]
    }
   ],
   "source": [
    "#new from 8.11.2018: 2017-65 Financiers of the Coal Exit List - final dataset 171207 - for media\n",
    "#with tabs: BH_SH_inv (investment) & LN_SI_BI_fin (financing)\n",
    "#turned into: urgewald_BH_SH_inv.csv (inv)& urgewald_LN_SI_BI_fin.csv (fin)\n",
    "\n",
    "#for 2017-65\n",
    "df_fin_2017 = pd.read_csv('.//ds_urgewald_LN_SI_BI_fin.csv', sep=';', encoding='latin-1',decimal=',', thousands='.', na_filter=False)\n",
    "\n",
    "df_fin_2017 = df_fin_2017[df_fin_2017['Final analysis?']=='Yes'] #That's how urgewald did it\n",
    "df_fin_2017 = df_fin_2017[df_fin_2017['International Finance Institution?']!='Yes']#remove IFIs\n",
    "\n",
    "\n",
    "df_fin_2017.drop(columns={'Ultimate Parent','Ultimate Parent Country','Deal Number / Deal ID','Borrower/Issuer',\n",
    "                    'Borrower/Issuer Country','Closing / Issue / Filing Date','Original Currency','Maturity Date',\\\n",
    "                    'Principal / Tranche Amount (original currency)','Total Package Amount (original currency)',\n",
    "                    'Principal/Tranche Amount (in mln USD)','Total Package Amount (in mln US$)','Use of Proceeds',\n",
    "                    'Manager Name','Manager Role / Participant Type','Percentage of Principal / Tranche Amount',\n",
    "                    'Source','Database','Final analysis?','International Finance Institution?',\n",
    "                    'Type of Financing (long)', 'Company/other financing'},inplace=True)\n",
    "\n",
    "df_fin_2017.rename(columns={'Group':'DEV', 'Group Country':'DEV Country', 'Investor Parent':'FIN',\n",
    "                      'Investor Parent Country':'FIN Country', 'Per Investor Value (in mln USD)':'Grand Total'}, \\\n",
    "              inplace=True)\n",
    "\n",
    "df_fin_2017.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for x in df_fin_2017.index:\n",
    "    if df_fin_2017.loc[x, 'FIN Country']=='Russian Federation':\n",
    "        df_fin_2017.loc[x, 'FIN Country']='Russia'\n",
    "    if df_fin_2017.loc[x, 'DEV Country']=='Russian Federation':\n",
    "        df_fin_2017.loc[x, 'DEV Country']='Russia'\n",
    "    if df_fin_2017.loc[x, 'FIN Country']=='':\n",
    "        df_fin_2017.loc[x, 'FIN Country']='unknown'\n",
    "    if df_fin_2017.loc[x, 'DEV Country']=='':\n",
    "        df_fin_2017.loc[x, 'DEV Country']='unknown'\n",
    "\n",
    "df_fin_2017_abr = df_fin_2017.copy()\n",
    "for x in df_fin_2017_abr.index:\n",
    "    if df_fin_2017_abr.loc[x,'DEV Country']==df_fin_2017_abr.loc[x,'FIN Country']:\n",
    "        df_fin_2017_abr.loc[x, 'abroad']=0\n",
    "    else:\n",
    "        df_fin_2017_abr.loc[x, 'abroad']=1 \n",
    "\n",
    "print('Grand Total is '+ str(df_fin_2017_abr['Grand Total'].sum()))\n",
    "\n",
    "df_fin_2017_abr.to_csv('.//for_paper//df_overview//df_fin_2017_abr.csv', decimal=',', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mann\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grand Total is 1422447.95\n"
     ]
    }
   ],
   "source": [
    "#new from 24.2.2020: Dataset 2019-084 Coal financiers - urgewald - streamlined 191211.xlsm\n",
    "# turned into ds_coal_financiers_loans_underwriting_2019.csv and ds_coal_financiers_bond_shareholding_2019.csv\n",
    "df_fin_2019 = pd.read_csv('.//ds_coal_financiers_loans_underwriting_2019.csv', sep=';', \\\n",
    "                     encoding='latin-1',decimal=',', thousands='.')\n",
    "\n",
    "#first remove IFIs:\n",
    "df_fin_2019 = df_fin_2019[df_fin_2019['IFI']!='Yes'].reset_index(drop=True)\n",
    "#20200427: world bank & EIB still in there!!? WHy?\n",
    "\n",
    "df_fin_2019.drop(columns={'Ultimate Parent Name','Ultimate Parent Country','Deal Number / Deal ID','Borrower/Issuer Name',\\\n",
    "                    'Borrower / Issuer Country','Closing / Issue / Filing Date', 'Maturity Date', 'Type of financing',\\\n",
    "                    'Principal / Tranche Amount(in mln of original currency)', 'Original currency',\\\n",
    "                    'Principal / Tranche Amount (in mln US$)','Total Package Amount (in mln of original currency)',\\\n",
    "                    'Total Package Amount (in mln US$)', 'Use of Proceeds','Manager Name', \\\n",
    "                    'Manager role / Participant type','Percentage of Principal / Tranche Amount', 'Source',\\\n",
    "                    'Database','CPDL company scope','IFI', 'Project finance',},inplace=True)\n",
    "\n",
    "df_fin_2019.rename(columns={'Group':'DEV', 'Group country':'DEV Country', 'Investor Parent':'FIN',\n",
    "                      'Investor Parent Country':'FIN Country', 'Per Investor Value (in mln US$)':'Grand Total', \n",
    "                      'Type short':'Type of Financing (short)'}, \\\n",
    "              inplace=True)\n",
    "#change from Grant to Grand \n",
    "\n",
    "#P: unsupported operand type(s) for +: 'float' and 'str'\n",
    "#S: remove na_filter=False for pd.read_csv\n",
    "\n",
    "#P: nan in all columns!? the last 10 rows or so all nan.\n",
    "#S: dropna\n",
    "df_fin_2019.dropna(axis='columns', how='all')\n",
    "\n",
    "df_fin_2019_abr = df_fin_2019.copy()\n",
    "for x in df_fin_2019_abr.index:\n",
    "    if df_fin_2019_abr.loc[x,'DEV Country']==df_fin_2019_abr.loc[x,'FIN Country']:\n",
    "        df_fin_2019_abr.loc[x, 'abroad']=0\n",
    "    else:\n",
    "        df_fin_2019_abr.loc[x, 'abroad']=1 \n",
    "\n",
    "print('Grand Total is '+ str(df_fin_2019_abr['Grand Total'].sum()))\n",
    "\n",
    "df_fin_2019_abr.to_csv('.//for_paper//df_overview//df_fin_2019_abr.csv', decimal=',', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the two datasets, take 2012&2013 from df_fin_2017 and merge with df_fin_2019\n",
    "df_fin_2012_2013_abr = df_fin_2017_abr[(df_fin_2017_abr['Year']==2012) | (df_fin_2017_abr['Year']==2013)]\\\n",
    ".reset_index(drop=True)\n",
    "df_fin_2012_2019_abr = pd.concat([df_fin_2012_2013_abr, df_fin_2019_abr], axis=0, join='outer', ignore_index=True)\n",
    "\n",
    "df_fin_2012_2019_abr.to_csv('.//for_paper//df_overview//df_fin_2012_2019_abr.csv', decimal=',', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bond & shareholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Share Total is 276154.99\n"
     ]
    }
   ],
   "source": [
    "#for 2019: ds_coal_financiers_bond_shareholding_2019.csv\n",
    "\n",
    "df_inv_2019 = pd.read_csv('.//ds_coal_financiers_bond_shareholding_2019.csv', sep=';', \\\n",
    "                          encoding='latin-1',decimal=',', thousands='.', na_filter=False)\n",
    "\n",
    "df_inv_2019.drop(columns={'Ultimate Parent Name', 'Ultimate Parent Country', 'Deal Number / Deal ID', \\\n",
    "                     'Borrower/Issuer Name', 'Borrower / Issuer Country', 'Closing / Issue / Filing Date',\\\n",
    "                     'Maturity Date','Type of financing', 'Interest Rate / Coupon', 'Borrower / Issuer CUSIP', \\\n",
    "                     'ISIN', 'Manager Name', 'Manager Country', 'Type of shares', 'Number of shares',\\\n",
    "                     'Percentage of shares or total debt outstanding','Source', 'Database', 'CPDL company scope', \\\n",
    "                     'IFI' },inplace=True)\n",
    "\n",
    "df_inv_2019.rename(columns={'Group':'DEV', 'Group country':'DEV Country', 'Investor Parent':'INV', \\\n",
    "                      'Investor Parent Country':'INV Country', 'Per Investor Value (in mln US$)':'Share Total', \\\n",
    "                      'Type short':'Type of financing (short)'}, inplace=True)\n",
    "\n",
    "df_inv_2019['Type of financing (short)'] = \\\n",
    "df_inv_2019['Type of financing (short)'].str.lower()\n",
    "\n",
    "df_inv_2019_abr = df_inv_2019.copy()\n",
    "\n",
    "for x in df_inv_2019_abr.index:\n",
    "    if df_inv_2019_abr.loc[x,'DEV Country']==df_inv_2019_abr.loc[x,'INV Country']:\n",
    "        df_inv_2019_abr.loc[x, 'abroad']=0\n",
    "    else:\n",
    "        df_inv_2019_abr.loc[x, 'abroad']=1 \n",
    "\n",
    "print('Share Total is '+ str(df_inv_2019_abr['Share Total'].sum()))\n",
    "        \n",
    "df_inv_2019_abr.to_csv('.//for_paper//df_overview//df_inv_2019_abr.csv', decimal=',', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get the headquarters of Developers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first: match with existing list of DEV and headquarter (that I got with API search ealier):\n",
    "df_dev_HQ = pd.read_csv('.//for_paper//df_overview//dev_HQ.csv', sep=';', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mann\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (8,14,15,31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df_dev_split = pd.read_csv('.//for_paper//df_overview//df_dev_split_'+z+'.csv', decimal=',', sep=';', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_split['DEV Country'] = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.87%"
     ]
    }
   ],
   "source": [
    "for x in df_dev_split.index:\n",
    "    for y in df_dev_HQ.index:\n",
    "        if df_dev_split.loc[x, 'DEV'] == df_dev_HQ.loc[y, 'DEV']:\n",
    "            df_dev_split.loc[x, 'DEV Country'] = df_dev_HQ.loc[y, 'DEV Country']\n",
    "            print('\\r'+ '%.2f' % (x/len(df_dev_split)*100) + '%', end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P: in fin&inv: 'United Kingdom'; in dev: 'England'&'United Kingdom'\n",
    "#S: change in dev to UK\n",
    "for y in ['DEV Country', 'Site Country']:\n",
    "    for x in df_dev_split.index:\n",
    "        if df_dev_split.loc[x, y]=='England':\n",
    "            df_dev_split.loc[x, y]='United Kingdom'\n",
    "    for x in df_dev_split.index:\n",
    "        if df_dev_split.loc[x, y]=='England':\n",
    "            df_dev_split.loc[x, y]='United Kingdom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_split_HQ = df_dev_split.copy()\n",
    "df_dev_split_HQ.to_csv('.//for_paper//df_overview//df_dev_split_HQ_'+z+'.csv', decimal=',', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capacity unknown = 1538870\n",
      "Capacity known = 3146099\n"
     ]
    }
   ],
   "source": [
    "#Calulate: How much known?\n",
    "mask1 = df_dev_split_HQ['DEV Country']=='unknown'\n",
    "mask2 = df_dev_split_HQ['DEV Country']!='unknown'\n",
    "print('Capacity unknown = '+ str(df_dev_split_HQ[mask1]['Capacity (MW)'].sum()))\n",
    "print('Capacity known = '+ str(df_dev_split_HQ[mask2]['Capacity (MW)'].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEV</th>\n",
       "      <th>DEV Country</th>\n",
       "      <th>Capacity (MW)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>65121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>933</td>\n",
       "      <td>Adhunik Group</td>\n",
       "      <td>unknown</td>\n",
       "      <td>6390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>987</td>\n",
       "      <td>Videocon Industries</td>\n",
       "      <td>unknown</td>\n",
       "      <td>6360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1318</td>\n",
       "      <td>Italian-Thai Development</td>\n",
       "      <td>unknown</td>\n",
       "      <td>6135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>934</td>\n",
       "      <td>CSPGCL</td>\n",
       "      <td>unknown</td>\n",
       "      <td>6100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           DEV DEV Country  Capacity (MW)\n",
       "13                     unknown     unknown          65121\n",
       "933              Adhunik Group     unknown           6390\n",
       "987        Videocon Industries     unknown           6360\n",
       "1318  Italian-Thai Development     unknown           6135\n",
       "934                     CSPGCL     unknown           6100"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#group by dev to see which important HQs are missing\n",
    "mask = df_dev_split_HQ['DEV Country']=='unknown'\n",
    "df_dev_split_HQ[mask].groupby(['DEV', 'DEV Country'], sort=False,as_index=False)[['Capacity (MW)']].sum()\\\n",
    ".sort_values('Capacity (MW)', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for after 2015:\n",
    "df_dev_split_HQ_2015_onwards = df_dev_split_HQ[(df_dev_split_HQ['Year']>=2015) | \\\n",
    "                                                                         (df_dev_split_HQ['Year']==0)]\n",
    "df_dev_split_HQ_2015_onwards_wo_can = df_dev_split_HQ_2015_onwards\\\n",
    "[df_dev_split_HQ_2015_onwards['Status']!='Cancelled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Country unknown = 365299\n",
      "out of total 1182616\n"
     ]
    }
   ],
   "source": [
    "#check how much unknown:\n",
    "mask = df_dev_split_HQ_2015_onwards_wo_can['DEV Country']=='unknown'\n",
    "print('DEV Country unknown = '+ str(df_dev_split_HQ_2015_onwards_wo_can[mask]['Capacity (MW)'].sum()))\n",
    "print('out of total ' + str(df_dev_split_HQ_2015_onwards_wo_can['Capacity (MW)'].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEV</th>\n",
       "      <th>DEV Country</th>\n",
       "      <th>Capacity (MW)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>32709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>452</td>\n",
       "      <td>IPP</td>\n",
       "      <td>unknown</td>\n",
       "      <td>3230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>537</td>\n",
       "      <td>Aq Sora</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>Genting Group</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>401</td>\n",
       "      <td>Athena Energy Ventures</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        DEV DEV Country  Capacity (MW)\n",
       "41                  unknown     unknown          32709\n",
       "452                     IPP     unknown           3230\n",
       "537                 Aq Sora     unknown           2640\n",
       "221           Genting Group     unknown           2620\n",
       "401  Athena Energy Ventures     unknown           2520"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#group by dev to see which important HQs are missing after 2015\n",
    "mask = df_dev_split_HQ_2015_onwards_wo_can['DEV Country']=='unknown'\n",
    "df_dev_split_HQ_2015_onwards_wo_can[mask].groupby(['DEV', 'DEV Country'], sort=False,as_index=False)[['Capacity (MW)']].sum()\\\n",
    ".sort_values('Capacity (MW)', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add manually to df_dev_HQ:\n",
    "dict_man_dev_HQ = {'China Energy Engineering Corporation':'China', 'Y?ld?r?m Group':'Turkey', \\\n",
    "                  'GCL Poly Energy Holdings':'China'}\n",
    "for x in dict_man_dev_HQ.keys():\n",
    "    df_dev_HQ = df_dev_HQ.append({'DEV':x, 'DEV Country':dict_man_dev_HQ[x]}, ignore_index=True)\n",
    "    \n",
    "df_dev_HQ.to_csv('.//for_paper//df_overview//dev_HQ.csv', sep=';')\n",
    "\n",
    "#then start from 'df_dev_HQ = pd.read_csv('.//tables//dev_HQ.csv', sep=';', index_col=0)'' again!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#24.4.2020: I manually changed HQ of Inner Mongoloa to China in dev_HQ in folder tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ealier: Do Google API search:\n",
    "\n",
    "#create df with all the DEV where no HQ is known till now!\n",
    "mask1 = df_dev_split_2019_01_HQ['DEV Country']=='unknown'\n",
    "df_dev_split_2019_01_HQ_unknown = df_dev_split_2019_01_HQ[mask1]\n",
    "\n",
    "df_dev_split_2019_01_HQ_unknown.reset_index(inplace=True, drop=True)\n",
    "d =  {'DEV':list(set(df_dev_split_2019_01_HQ_unknown['DEV']))}\n",
    "df_HQ_unknown = pd.DataFrame(data=d)\n",
    "df_HQ_unknown[\"placeHQ\"] = ''\n",
    "df_HQ_unknown[\"latitudeHQ\"] = ''\n",
    "df_HQ_unknown[\"longitudeHQ\"] = ''\n",
    "df_HQ_unknown[\"addressHQ\"] = ''\n",
    "\n",
    "df_HQ_unknown.replace({np.nan:'unknown'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "missing = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carbonifera Metropolitan headquarter\n",
      "[]\n",
      "\n",
      "\n",
      "Other headquarter\n",
      "name 'urllib' is not defined\n",
      "\n",
      "\n",
      "\n",
      "i = 7, DEV name = Other\n",
      "for 7 DEV: missing 6, got data for 1\n"
     ]
    }
   ],
   "source": [
    "df_HQ_unknown\n",
    "\n",
    "#get the place name, longitude & latitude for the missing DEV`s using google API\n",
    "\n",
    "#to get new API`s: https://developers.google.com/places/web-service/get-api-key\n",
    "#to enable API`s & create new projects etc: https://console.developers.google.com/cloud-resource-manager\n",
    "#AIzaSyAbM9f8tZzA03PBCBuQLXyKldKG5A9N_v4\n",
    "#YOUR_API_KEY = 'AIzaSyDnhtfR2g1_vzyAwLzidqzqGRYvyHv1mBI' #project2\n",
    "YOUR_API_KEY = 'AIzaSyAbM9f8tZzA03PBCBuQLXyKldKG5A9N_v4' #project2\n",
    "\n",
    "\n",
    "#https://maps.googleapis.com/maps/api/place/textsearch/xml?query=xxxxx+yyyyyyy+zzzzzzz&key=YOUR_API_KEY\n",
    "google_places = GooglePlaces(YOUR_API_KEY)\n",
    "\n",
    "#create list of parent names\n",
    "companies = df_HQ_unknown['DEV'][i:len(df_HQ_unknown['DEV'])]\n",
    "\n",
    "try:\n",
    "\n",
    "    for co in companies:\n",
    "\n",
    "        #with headquarter\n",
    "        print(str(co)+' headquarter')\n",
    "        try:\n",
    "            query_result = google_places.text_search(query=co+\" headquarter\")\n",
    "\n",
    "            if query_result.places == []:\n",
    "                k = 1\n",
    "\n",
    "            else:\n",
    "                for place in query_result.places:\n",
    "\n",
    "                    df_HQ_unknown.loc[i, \"placeHQ\"] = place.name\n",
    "                    df_HQ_unknown.loc[i, \"latitudeHQ\"] = float(place.geo_location['lat'])\n",
    "                    df_HQ_unknown.loc[i, \"longitudeHQ\"] = float(place.geo_location['lng'])\n",
    "\n",
    "                    #for the formatted_address you have to get details\n",
    "                    place.get_details()\n",
    "                    df_HQ_unknown.loc[i, \"addressHQ\"] = place.formatted_address\n",
    "\n",
    "                    k = 0\n",
    "\n",
    "            print(query_result.places)\n",
    "            \n",
    "            \n",
    "        except urllib.error.HTTPError:\n",
    "            print('PlaceSearchResponse><status>UNKNOWN_ERROR')\n",
    "            k = 1\n",
    "\n",
    "            \n",
    "        print(\"\\n\")\n",
    "        \n",
    "        if k == 1:\n",
    "            missing += 1\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    print('for ' + str(i) + ' DEV: missing ' + str(missing) + ', got data for ' + str(i-missing))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e: \n",
    "    print(str(e)+\"\\n\"+\"\\n\"+\"\\n\")\n",
    "\n",
    "print('i = ' + str(i) + ', DEV name = ' + co)\n",
    "print('for ' + str(i) + ' DEV: missing ' + str(missing) + ', got data for ' + str(i-missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add pipeline and abroad to df_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Announced',\n",
       " 'Cancelled',\n",
       " 'Construction',\n",
       " 'Mothballed',\n",
       " 'Operating',\n",
       " 'Permitted',\n",
       " 'Pre-Permit',\n",
       " 'Retired',\n",
       " 'Shelved'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add 'pipeline'\n",
    "df_dev_split_HQ = pd.read_csv('.//for_paper//df_overview//df_dev_split_HQ_'+z+'.csv', \\\n",
    "                              decimal=',', sep=';', index_col=0)\n",
    "df_dev_split_HQ_with_pipe = df_dev_split_HQ.copy()\n",
    "set(df_dev_split_HQ_with_pipe['Status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pipeline = ['Announced', 'Construction', 'Permitted', 'Pre-Permit']#it's actually Pre-Permit (uppercase Ps)\n",
    "list_pipeline_wo_const = ['Announced', 'Permitted', 'Pre-Permit']\n",
    "\n",
    "df_dev_split_HQ_with_pipe['Pipeline'] = 0\n",
    "df_dev_split_HQ_with_pipe['Pipeline wo const'] = 0\n",
    "\n",
    "for x in df_dev_split_HQ_with_pipe.index:\n",
    "    if df_dev_split_HQ_with_pipe.loc[x, 'Status'] in list_pipeline:\n",
    "        df_dev_split_HQ_with_pipe.loc[x, 'Pipeline'] = 1\n",
    "    if df_dev_split_HQ_with_pipe.loc[x, 'Status'] in list_pipeline_wo_const:\n",
    "        df_dev_split_HQ_with_pipe.loc[x, 'Pipeline wo const'] = 1\n",
    "        \n",
    "df_dev_split_HQ_with_pipe.to_csv('.//for_paper//df_overview//df_dev_split_HQ_with_pipe_'+z+'.csv', \\\n",
    "                                         decimal=',', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipe MW = 499097.00\n",
      "planned MW= 299554.00\n",
      "constr MW = 199543.00\n"
     ]
    }
   ],
   "source": [
    "print('Pipe MW = '+\"%.2f\" %(df_dev_split_HQ_with_pipe[df_dev_split_HQ_with_pipe['Pipeline'] == 1]['Capacity (MW)'].sum()))\n",
    "print('planned MW= '+\"%.2f\" %(df_dev_split_HQ_with_pipe[df_dev_split_HQ_with_pipe['Pipeline wo const'] == 1]['Capacity (MW)'].sum()))\n",
    "const = (df_dev_split_HQ_with_pipe[df_dev_split_HQ_with_pipe['Status'] == 'Construction']['Capacity (MW)'].sum())\n",
    "print('constr MW = '+\"%.2f\" %const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add abroad\n",
    "df_dev_split_HQ_with_pipe_abr = pd.read_csv('.//for_paper//df_overview//df_dev_split_HQ_with_pipe_'+z+'.csv', \\\n",
    "                                            decimal=',', sep=';', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_split_HQ_with_pipe_abr['abroad'] = 0\n",
    "for x in df_dev_split_HQ_with_pipe_abr.index:\n",
    "    if df_dev_split_HQ_with_pipe_abr.loc[x, 'DEV Country']!= \\\n",
    "    df_dev_split_HQ_with_pipe_abr.loc[x, 'Site Country']:\n",
    "        df_dev_split_HQ_with_pipe_abr.loc[x, 'abroad'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abroad = 1957589\n",
      "domestic = 2727380\n"
     ]
    }
   ],
   "source": [
    "mask1 = df_dev_split_HQ_with_pipe_abr['abroad']==1\n",
    "mask2 = df_dev_split_HQ_with_pipe_abr['abroad']==0\n",
    "print('abroad = '+str(df_dev_split_HQ_with_pipe_abr[mask1]['Capacity (MW)'].sum()))\n",
    "print('domestic = '+str(df_dev_split_HQ_with_pipe_abr[mask2]['Capacity (MW)'].sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_split_HQ_with_pipe_abr.to_csv('.//for_paper//df_overview//df_dev_split_HQ_with_pipe_abr_'+z+'.csv', \\\n",
    "                                             decimal=',', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Regions to datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use regions from IPCC (Krey et al., ISBN: 978-1-107-65481-5)\n",
    "dict_regions={}\n",
    "dict_regions['North America'] =  ['Canada', 'Guam', 'Saint Pierre and Miquelon', 'United States']\n",
    "dict_regions['Western Europe'] =  ['Aland Islands', 'Andorra', 'Austria', 'Belgium',                                    'Channel Islands', 'Denmark', 'Faroe Islands', 'Finland', 'France', \n",
    "                                   'Germany', 'Gibraltar', 'Greece', 'Greenland', 'Guernsey', \n",
    "                                   'Holy See (Vatican City State)', 'Iceland', 'Ireland', 'Isle of Man', \n",
    "                                   'Italy', 'Jersey', 'Liechtenstein', 'Luxembourg', 'Monaco', 'Netherlands', \n",
    "                                   'Norway', 'Portugal', 'San Marino', 'Spain', 'Svalbard and Jan Mayen', \n",
    "                                   'Sweden', 'Switzerland', 'United Kingdom', 'Turkey',  'England']\n",
    "dict_regions['Pacific OECD'] =   ['Australia', 'Japan', 'New Zealand']\n",
    "dict_regions['Economies in Transition'] =   ['Croatia', 'Cyprus', 'Czech Republic',  'Kosovo', 'Estonia', \n",
    "                                             'Latvia', 'Lithuania', 'Malta', 'Poland', 'Russia', 'Slovakia', \n",
    "                                             'Slovenia', 'Kyrgyzstan', 'Tajikistan', 'Armenia', 'Georgia', \n",
    "                                             'Moldova', 'Ukraine', 'Uzbekistan', 'Albania', 'Azerbaijan', \n",
    "                                             'Belarus', 'Bosnia and Herzegovina', 'Bulgaria', 'Hungary', \n",
    "                                             'Kazakhstan', 'FYROM', 'Montenegro', 'Romania', 'Serbia', \n",
    "                                             'Serbia and Montenegro', 'Turkmenistan;', 'Czechia', 'Kazakhstan ',\n",
    "                                             'Bosnia & Herzegovina','North Macedonia']\n",
    "dict_regions['Latin America and Caribbean'] =   ['Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Aruba', \n",
    "     'Bahamas', 'Barbados', 'Bermuda', 'Bouvet Island', 'British Virgin Islands', 'Cayman Islands', 'Chile', \n",
    "    'Curacao', 'Falkland Islands (Malvinas)', 'French Guiana', 'French Southern Territories', 'Guadeloupe', \n",
    "     'Martinique', 'Montserrat', 'Netherlands Antilles', 'Puerto Rico', 'Saint Kitts and Nevis', 'Sint Maarten', \n",
    "     'South Georgia and the South Sandwich Islands', 'Trinidad and Tobago', 'Turks and Caicos Islands', 'Uruguay', \n",
    "     'US Virgin Islands', 'Haiti', 'Bolivia', 'El Salvador', 'Guatemala', 'Guyana', 'Honduras', 'Nicaragua', \n",
    "     'Paraguay', 'Argentina', 'Belize', 'Brazil', 'Colombia', 'Costa Rica', 'Cuba', 'Dominica', \n",
    "     'Dominican Republic', 'Ecuador', 'Grenada', 'Jamaica', 'Mexico', 'Panama', 'Peru', 'Saint Lucia', \n",
    "     'Saint Vincent and the Grenadines', 'Suriname', 'Venezuela']\n",
    "dict_regions['Sub Saharan Africa'] =   ['Equatorial Guinea', 'Mayotte', 'Reunion', 'Saint Helena', 'Benin', \n",
    "        'Burkina Faso', 'Burundi', 'Central African Republic', 'Chad', 'Comoros', 'Democratic Republic of Congo', \n",
    "        'Eritrea', 'Ethiopia', 'Gambia', 'Guinea', 'Guinea-Bissau', 'Kenya', 'Liberia', 'Madagascar', 'Malawi', \n",
    "        'Mali', 'Mozambique', 'The Niger', 'Rwanda', 'Sierra Leone', 'Somalia', 'Tanzania', 'Togo', 'Uganda', \n",
    "        'Zimbabwe', 'Cameroon', 'Cape Verde', 'Congo', 'Ivory Coast', 'Djibouti', 'Ghana', 'Lesotho', \n",
    "        'Mauritania', 'Nigeria', 'Sao Tome and Principe', 'Senegal', 'Swaziland', 'Zambia', 'Angola', \n",
    "        'Botswana', 'Gabon', 'Mauritius', 'Namibia', 'Seychelles', 'South Africa', 'Niger']\n",
    "dict_regions['Middle East and North Africa'] =  ['Bahrain', 'Israel', 'Africa','Kuwait', 'Oman', 'Qatar', \n",
    "         'Saudi Arabia', 'United Arab Emirates', 'Egypt', 'Morocco', 'Palestine', 'South Sudan', 'Sudan', \n",
    "         'Syria', 'Western Sahara', 'Yemen', 'Algeria', 'Iran', 'Iraq','Dubai', 'Jordan', 'Lebanon', 'Libya', \n",
    "         'Tunisia']\n",
    "dict_regions['East Asia'] =  ['South Korea', 'North Korea', 'Mongolia', 'China', 'Hong Kong', 'Taiwan',\n",
    "                             'Hong Kong, China']\n",
    "dict_regions['South Asia'] =  ['British Indian Ocean Territory', 'Afghanistan', 'Bangladesh', 'Nepal', 'Bhutan', \n",
    "       'India', 'Pakistan', 'Sri Lanka', 'Maldives']\n",
    "dict_regions['South-East Asia and Pacific'] =  ['Brunei Darussalam', 'Christmas Island', \n",
    "        'Cocos (Keeling) Islands', 'French Polynesia', 'Heard Island and McDonald Islands', 'New Caledonia', \n",
    "        'Norfolk Island', 'Northern Mariana Islands', 'Pitcairn', 'Singapore', 'Tokelau', \n",
    "        'US Minor Outlying Islands', 'Wallis and Futuna', 'Cambodia', 'Myanmar', 'Indonesia', 'Kiribati', \n",
    "        'Laos', 'Micronesia (Federated States of)', 'Nauru', 'Papua New Guinea', 'Philippines', 'Samoa', \n",
    "        'Solomon Islands', 'Timor-Leste', 'Vanuatu', 'Vietnam', 'Niue', 'American Samoa', 'Cook Islands', 'Fiji', \n",
    "        'Malaysia', 'Marshall Islands', 'Palau', 'Thailand', 'Tonga', 'Tuvalu', 'Brunei']\n",
    "#-INT TRA (International transport): 'International Aviation', 'International Shipping\n",
    "\n",
    "\n",
    "#these had to be altered: Democratic Republic of Congo; Russia; Hong Kong; EU;Global;Dubai;Moldova;Germany;\n",
    "    #unknown;Africa;Vietnam;Taiwan;Ivory Coast;FYROM;Kosovo;Syria;The Niger;Laos\n",
    "#->I changed the names in the IPCC lists!\n",
    "#included: North Macedonia, Brunei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if all countries are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_region_dict(count_name):\n",
    "    var_name = count_name\n",
    "    for region_name in dict_regions.keys():\n",
    "        if count_name in dict_regions[region_name]:\n",
    "            var_name = 0\n",
    "    return(var_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown\n"
     ]
    }
   ],
   "source": [
    "#make list with all countries from df_fin & inv & dev:\n",
    "set_all_countries_df = set(df_fin_2012_2019_abr_reg['FIN Country']) | \\\n",
    "set(df_fin_2012_2019_abr_reg['DEV Country']) | set(df_inv_2019_abr_reg['DEV Country'])| \\\n",
    "    set(df_inv_2019_abr_reg['INV Country'])| set(df_dev_split_HQ_with_pipe_abr_reg['DEV Country'])| \\\n",
    "set(df_dev_split_HQ_with_pipe_abr_reg['Site Country'])\n",
    "\n",
    "for x in set_all_countries_df:\n",
    "    variable_name = in_region_dict(x)\n",
    "    if variable_name != 0:\n",
    "        print(variable_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### include region in each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region(count_name):\n",
    "    var_name = 0\n",
    "    for region_name in dict_regions.keys():\n",
    "        if count_name in dict_regions[region_name]:\n",
    "            var_name = region_name\n",
    "    if var_name!=0:\n",
    "        return(var_name)\n",
    "    else:\n",
    "        return(count_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_split_HQ_with_pipe_abr_reg = \\\n",
    "pd.read_csv('.//for_paper//df_overview//df_dev_split_HQ_with_pipe_abr_'+z+'.csv', decimal=',', sep=';', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin_2012_2019_abr_reg = pd.read_csv('.//for_paper//df_overview//df_fin_2012_2019_abr.csv', \\\n",
    "                                       decimal=',', sep=';', index_col=0)\n",
    "df_inv_2019_abr_reg = pd.read_csv('.//for_paper//df_overview//df_inv_2019_abr.csv', \\\n",
    "                                  decimal=',', sep=';', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add region of each country to datasets\n",
    "for x in df_dev_split_HQ_with_pipe_abr_reg.index:\n",
    "    for y in ['Site', 'DEV']:\n",
    "        df_dev_split_HQ_with_pipe_abr_reg.loc[x, y+' Region'] = \\\n",
    "        get_region(df_dev_split_HQ_with_pipe_abr_reg.loc[x, y+' Country'])\n",
    "        \n",
    "df_dev_split_HQ_with_pipe_abr_reg.to_csv('.//for_paper//df_overview//df_dev_split_HQ_with_pipe_abr_reg_'+z+'.csv', \\\n",
    "                                                 decimal=',', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get region and also check, if country name is in region name!\n",
    "for x in df_fin_2012_2019_abr_reg.index:\n",
    "    for y in ['FIN', 'DEV']:\n",
    "        df_fin_2012_2019_abr_reg.loc[x, y+' Region'] = get_region(df_fin_2012_2019_abr_reg.loc[x, y+' Country'])\n",
    "        if df_fin_2012_2019_abr_reg.loc[x, y+' Region'] == df_fin_2012_2019_abr_reg.loc[x, y+' Country']:\n",
    "            print(df_fin_2012_2019_abr_reg.loc[x, y+' Country'])\n",
    "        \n",
    "df_fin_2012_2019_abr_reg.to_csv('.//for_paper//df_overview//df_fin_2012_2019_abr_reg.csv', decimal=',', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get region and also check, if country name is in region name!\n",
    "for x in df_inv_2019_abr_reg.index:\n",
    "    for y in ['INV', 'DEV']:\n",
    "        df_inv_2019_abr_reg.loc[x, y+' Region'] = get_region(df_inv_2019_abr_reg.loc[x, y+' Country'])\n",
    "        if df_inv_2019_abr_reg.loc[x, y+' Region'] == df_inv_2019_abr_reg.loc[x, y+' Country']:\n",
    "            print(df_inv_2019_abr_reg.loc[x, y+' Country'])\n",
    "        \n",
    "df_inv_2019_abr_reg.to_csv('.//for_paper//df_overview//df_inv_2019_abr_reg.csv', decimal=',', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Annex l category to datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_annex_1 = ['Australia', 'Austria', 'Belarus', 'Belgium', 'Bulgaria', 'Canada', 'Croatia', 'Cyprus', \n",
    "                'Czech Republic', 'Denmark', 'Estonia', 'European Union', 'Finland', 'France', 'Germany', \n",
    "                'Greece', 'Hungary', 'Iceland', 'Ireland', 'Italy', 'Japan', 'Latvia', 'Liechtenstein', \n",
    "                'Lithuania', 'Luxembourg', 'Malta', 'Monaco', 'Netherlands', 'New Zealand', 'Norway', 'Poland', \n",
    "                'Portugal', 'Romania', 'Russian Federation', 'Slovakia', 'Slovenia', 'Spain', 'Sweden', \n",
    "                'Switzerland', 'Turkey', 'Ukraine', 'United Kingdom of Great Britain and Northern Ireland', \n",
    "                'United States of America','Guernsey', 'Moldova','Isle of Man','Channel Islands','Russia',\n",
    "                'Kosovo','United Kingdom','FYROM','United States','Jersey', 'England', 'British Virgin Islands']\n",
    "list_non_annex_1 = ['Afghanistan', 'Albania', 'Algeria', 'Andorra', 'Angola', 'Antigua and Barbuda', 'Argentina', \n",
    "                    'Armenia', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belize', 'Benin', \n",
    "                    'Bhutan', 'Bolivia (Plurinational State of)', 'Bosnia and Herzegovina', 'Botswana', 'Brazil', \n",
    "                    'Brunei Darussalam', 'Burkina Faso', 'Burundi', 'Cabo Verde', 'Cambodia', 'Cameroon', \n",
    "                    'Central African Republic', 'Chad', 'Chile', 'China', 'Colombia', 'Comoros', 'Congo', \n",
    "                    'Cook Islands', 'Costa Rica', 'Ivory Coast', 'Cuba', 'North Korea', \n",
    "                    'Democratic Republic of the Congo', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', \n",
    "                    'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Eswatini', 'Ethiopia', 'Fiji', \n",
    "                    'Gabon', 'Gambia', 'Georgia', 'Ghana', 'Grenada', 'Guatemala', 'Guinea', 'Guinea-Bissau', \n",
    "                    'Guyana', 'Haiti', 'Honduras', 'India', 'Indonesia', 'Iran', 'Iraq', 'Israel', 'Jamaica', \n",
    "                    'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', 'Kuwait', 'Kyrgyzstan', 'Laos', 'Lebanon', \n",
    "                    'Lesotho', 'Liberia', 'Libya', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', \n",
    "                    'Marshall Islands', 'Mauritania', 'Mauritius', 'Mexico', 'Micronesia (Federated States of)', \n",
    "                    'Mongolia', 'Montenegro', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', \n",
    "                    'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Oman', 'Pakistan', 'Palau', 'Panama', \n",
    "                    'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Qatar', 'Republic of Korea', \n",
    "                    'Republic of Moldova', 'Rwanda', 'Saint Kitts and Nevis', 'Saint Lucia', \n",
    "                    'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', \n",
    "                    'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', \n",
    "                    'Solomon Islands', 'Somalia', 'South Africa', 'South Sudan', 'Sri Lanka', \n",
    "                    'State of Palestine', 'Sudan', 'Suriname', 'Syrian Arab Republic', 'Tajikistan', \n",
    "                    'Thailand', 'The former Yugoslav Republic of Macedonia', 'Timor-Leste', 'Togo', 'Tonga', \n",
    "                    'Trinidad and Tobago', 'Tunisia', 'Turkmenistan', 'Tuvalu', 'Uganda', 'United Arab Emirates', \n",
    "                    'United Republic of Tanzania', 'Uruguay', 'Uzbekistan', 'Vanuatu', \n",
    "                    'Venezuela (Bolivarian Republic of)', 'Viet Nam', 'Yemen', 'Zambia', 'Zimbabwe','Africa',\n",
    "                    'Hong Kong','Swaziland','Tanzania','Cayman Islands','Dubai','Bermuda','Syria','Venezuela',\n",
    "                    'The Niger','Taiwan','Democratic Republic of Congo','Vietnam','South Korea',\n",
    "                    'Hong Kong, China', 'Niger','Czechia', 'Kazakhstan ', 'Bosnia & Herzegovina',\n",
    "                    'North Macedonia', 'Brunei']\n",
    "\n",
    "#-> I added to list_annex_1 : 'Guernsey', 'Moldova','Isle of Man','Channel Islands','Russia',\n",
    "    #'Kosovo','United Kingdom','FYROM','United States','Jersey'\n",
    "#added to list_non_annex_1 : 'Hong Kong','Swaziland','Tanzania','Cayman Islands','Dubai',\n",
    "    #'Bermuda','Syria','Venezuela','The Niger','Taiwan','Democratic Republic of Congo','Vietnam',\n",
    "    #'South Korea','unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_split_HQ_with_pipe_abr_reg_ann = pd.read_csv('.//for_paper//df_overview//df_dev_split_HQ_with_pipe_abr_reg_'+z+'.csv', \\\n",
    "                                                 decimal=',', sep=';', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin_2012_2019_abr_reg_ann = pd.read_csv('.//for_paper//df_overview//df_fin_2012_2019_abr_reg.csv', decimal=',', \\\n",
    "                                           sep=';', index_col=0)\n",
    "df_inv_2019_abr_reg_ann = pd.read_csv('.//for_paper//df_overview//df_inv_2019_abr_reg.csv', decimal=',', \\\n",
    "                                           sep=';', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if all countries are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown\n"
     ]
    }
   ],
   "source": [
    "#make list with all countries from df_fin & inv & dev:\n",
    "set_all_countries_df = set(df_fin_2012_2019_abr_reg_ann['FIN Country']) | \\\n",
    "    set(df_fin_2012_2019_abr_reg_ann['DEV Country']) | set(df_inv_2019_abr_reg_ann['DEV Country'])| \\\n",
    "    set(df_inv_2019_abr_reg_ann['INV Country'])| set(df_dev_split_HQ_with_pipe_abr_reg_ann['DEV Country'])| \\\n",
    "    set(df_dev_split_HQ_with_pipe_abr_reg_ann['Site Country'])\n",
    "\n",
    "for x in set_all_countries_df:\n",
    "    if x not in list_annex_1:\n",
    "        if x not in list_non_annex_1:\n",
    "            print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### include Region name in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_annex = {'annex_1':list_annex_1, 'non_annex_1':list_non_annex_1}\n",
    "\n",
    "def get_annex(count_name): \n",
    "    var_name = 0 \n",
    "    for annex_name in dict_annex.keys(): \n",
    "        if count_name in dict_annex[annex_name]: \n",
    "            var_name = annex_name \n",
    "    if var_name!=0: \n",
    "        return(var_name) \n",
    "    else: return('something went wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in df_dev_split_HQ_with_pipe_abr_reg_ann.index: \n",
    "    for y in ['Site', 'DEV']: \n",
    "        df_dev_split_HQ_with_pipe_abr_reg_ann.loc[x, y+' Annex 1'] = \\\n",
    "        get_annex(df_dev_split_HQ_with_pipe_abr_reg_ann.loc[x, y+' Country'])\n",
    "        \n",
    "df_dev_split_HQ_with_pipe_abr_reg_ann.to_csv('.//for_paper//df_overview//df_dev_'+z+'_final.csv',decimal=',', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in df_fin_2012_2019_abr_reg_ann.index: \n",
    "    for y in ['FIN', 'DEV']: \n",
    "        df_fin_2012_2019_abr_reg_ann.loc[x, y+' Annex 1'] = \\\n",
    "        get_annex(df_fin_2012_2019_abr_reg_ann.loc[x, y+' Country'])\n",
    "        \n",
    "df_fin_2012_2019_abr_reg_ann.to_csv('.//for_paper//df_overview//df_fin_2012_2019_final.csv', decimal=',', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in df_inv_2019_abr_reg_ann.index: \n",
    "    for y in ['INV', 'DEV']: \n",
    "        df_inv_2019_abr_reg_ann.loc[x, y+' Annex 1'] = \\\n",
    "        get_annex(df_inv_2019_abr_reg_ann.loc[x, y+' Country'])\n",
    "        \n",
    "df_inv_2019_abr_reg_ann.to_csv('.//for_paper//df_overview//df_inv_2019_final.csv', decimal=',', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total Capacity in final: 4684969\n",
      "total Grants in final: 1695397.8800000004\n",
      "total Share in final: 276154.99\n"
     ]
    }
   ],
   "source": [
    "#check for some data:\n",
    "print('total Capacity in final: '+ str(df_dev_split_HQ_with_pipe_abr_reg_ann['Capacity (MW)'].sum()))\n",
    "print('total Grants in final: '+ str(df_fin_2012_2019_abr_reg_ann['Grand Total'].sum()))\n",
    "print('total Share in final: '+ str(df_inv_2019_abr_reg_ann['Share Total'].sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# do dataframe with matched developers names to calculate the emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P: developers have different names across the datasets\n",
    "#S: -->Match them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import final datasets\n",
    "df_dev_2020_01_final = pd.read_csv('.//for_paper//df_overview//df_dev_'+z+'_final.csv',decimal=',', \\\n",
    "                                   sep=';', index_col=0)\n",
    "df_fin_2012_2019_final = pd.read_csv('.//for_paper//df_overview//df_fin_2012_2019_final.csv', \\\n",
    "                                     decimal=',', sep=';', index_col=0)\n",
    "df_inv_2019_final = pd.read_csv('.//for_paper//df_overview//df_inv_2019_final.csv', \\\n",
    "                                decimal=',', sep=';', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P: already in the fin dataset the DEV have diverging names (e.g. 'PLN (Persero)', 'PLN Persero')\n",
    "#S: change the DEV names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all devs as lower case!\n",
    "df_dev_2020_01_final_names = df_dev_2020_01_final.copy()\n",
    "df_dev_2020_01_final_names['DEV'] = df_dev_2020_01_final_names['DEV'].str.lower()\n",
    "df_fin_2012_2019_final_names = df_fin_2012_2019_final.copy()\n",
    "df_fin_2012_2019_final_names['DEV'] = df_fin_2012_2019_final_names['DEV'].str.lower()\n",
    "df_inv_2019_final_names = df_inv_2019_final.copy()\n",
    "df_inv_2019_final_names['DEV'] = df_inv_2019_final_names['DEV'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_DEV_names = {'AES Corporation':'AES', 'Aboitiz Equity Ventures': 'Aboitiz Group','Aboitiz Power':'Aboitiz Group', \\\n",
    "'Adaro Energy': 'Adaro', \\\n",
    "'Anhui Province Energy Group (Wenergy Group)':'Wenergy Group', 'Anhui Province Energy Group':'Wenergy Group', \\\n",
    "'banpu public company':'banpu', 'beijing guotai xinhua industry':'beijing energy holding', \\\n",
    "'bereket enerji retim':'bereket enerji', 'PT Bukit Asam':'Bukit Asam', \\\n",
    "'Beijing Energy Investment Holding':'Beijing Energy Holding' , 'Bihar State Power Holding (BSPHCL)':'BSPHCL', \\\n",
    "'CEZ':'CEZ Group', 'CLP Holdings':'CLP Group', \\\n",
    "'chifeng urban construction investment group':'chifeng economic development zone power generation', \\\n",
    "'china national coal group corp (chinacoal)' : 'chinacoal', 'china petrochemical group (sinopec group)':'sinopec', \\\n",
    "'china national machinery industry corp (sinomach)':\"lu'an group\", \\\n",
    "'china guodian group':'National Energy Investment Group', \\\n",
    "'China Datang':'China Datang Group', \\\n",
    "'China Datang Corporation': 'China Datang Group', 'China Energy Engineering Corporation':'China Energy Group', \\\n",
    "'China Energy Engineering Group (Energy China Group)':'China Energy Group', \\\n",
    "'China Energy Investment Corporation (China Energy)':'China Energy Group', \\\n",
    "'China Huadian': 'China Huadian Group', 'Centum Investment':'Centum Investment Group', \\\n",
    "'China Resources Power' : 'China Resources', 'China Huaneng Group':'China Huaneng', \\\n",
    "'Daewoo E&C':'Daewoo', 'dongfang electric corporation (dec)':'dongfang electric corporation', \\\n",
    "'datong coal mining group':'datong coal mine group', 'emco':'emco energy',\\\n",
    "'eurasian resources group (erg)':'eurasian resources group', \\\n",
    "'Eskom Holdings':'Eskom', 'Essar Energy':'Essar Group', 'Electricity Generating Authority of Thailand (EGAT)':'EGAT', \\\n",
    "'Elektroprivreda BIH (EPBIH)': 'Elektroprivreda BiH', \\\n",
    "'Elektroprivreda Srbije (EPS)':'Elektroprivreda Republike Srpske','pacificcorp':'first pacific co', \\\n",
    "'inner mongolia erdos investment holding group':'erdos hongjun electric power', \\\n",
    "'fortum oyj':'fortum', 'gmr infrastructure ltd':'gmr group', \\\n",
    "'gansu province electric power investment group (gepic)': 'gansu electric power investment group', \\\n",
    "'guangdong holdings':'guangdong energy group', \\\n",
    "'guangdong pearl river investment management':'pearl river investment', \\\n",
    "'harbin electric corporation':'harbin electric', 'bhimasena power indo (bpi - joint venture)':'indo power', \\\n",
    "'india power corporation': 'india power corp', 'asahan aluminium (inalum)':'indonesia asahan aluminium',\\\n",
    "'IL&FS Energy Development Company Limited (IEDCL)':'IL&FS', \\\n",
    "'Intra Energy Corporation (IEC)':'Intra Energy','J-POWER (Electric Power Development Company)':'J-POWER', \\\n",
    "'itochu enex':'itochu corporation', \\\n",
    "'J-Power (Electric Power Development Co)':'J-POWER', 'JSW Energy':'JSW Group', \\\n",
    "'Jindal Steel & Power (JSPL)':'Jindal Group', 'Jindal Steel & Power': 'Jindal Group' , \\\n",
    "'Jiuquan Iron & Steel (Group)':'Jiuquan Iron & Steel Group', \\\n",
    "'jiangxi provincial investment group':'jiangxi province investment group', \\\n",
    "'jp elektroprivreda bih':'jp elektroprivreda hzhb mostar', \\\n",
    "'Kansai Electric Power Co (Kepco)':'Kansai Electric Power (Kepco)', \\\n",
    "'kansai electric power': 'kansai electric power (kepco)', \\\n",
    "'kepco':'kansai electric power (kepco)', 'Korean South Eastern Power':'korea electric power (kepco)', \\\n",
    "'korea electric power (kepco) ':'korea electric power (kepco)', \\\n",
    "'Korea Electric Power Company (KEPCO)':'Korea Electric Power (KEPCO)', 'Kibo Mining':'Kibo Energy', \\\n",
    "'kobe steel ltd':'kobe steel', 'lucky cement limited':'lucky cement', \\\n",
    "'magna resources corporation':'magna resources', 'mitsui & co':'mitsui', \\\n",
    "'Manila Electric Co (Meralco)': 'MERALCO', 'Manila Electric Company (Meralco)':'MERALCO', \\\n",
    "'Neyveli Lignite Corporation (NLC India)':'NLC India', 'national aluminium co (nalco)':'nalco', \\\n",
    "'National Thermal Power Corporation (NTPC)':'NTPC', 'PLN (Persero)': 'PLN Persero', \\\n",
    "'national electricity corporation':'nepco', \\\n",
    "'Polska Grupa Energetyczna (PGE)':'PGE', 'Power Construction Corp of China (Powerchina)': 'Powerchina', \\\n",
    "'PowerChina (Power Construction Corporation of China)':'PowerChina', \\\n",
    "'Public Power Corporation (PPC)':'Public Power Corporation', 'Public Power Corp.':'Public Power Corporation', \\\n",
    "'Prophecy Development Corporation':'Prophecy Development', 'PT PLN Persero':'PLN Persero', \\\n",
    "'rajasthan rajya vidyut utpadan nigam limited':'rajasthan rv utpadan nigam', \\\n",
    "'Rajasthan Rajya Vidyut Utpadan Nigam':'Rajasthan RV Utpadan Nigam', 'Resgen':'Resource Generation', \\\n",
    "'reliance power':'reliance group', 'rural power company limited (rpcl)':'rural power company limited', \\\n",
    "'rpsg group / haldia energy':'hiranmaye energy', \\\n",
    "'inner mongolia shuangxin energy & chemical':'shuangxin energy group', \\\n",
    "'Shanghai Electric Group Corp':'Shanghai Electric Group', \\\n",
    "'Shandong Weiqiao Pioneering Group':'Shandong Weiqiao Group', \\\n",
    "'SMC Global Power Holdings Corporation':'SMC Global Power Holdings', \\\n",
    "'State Development and Investment Corporation (SDIC)':'SDIC', 'State Grid Corporation of China (SGCC)':'SGCC', \\\n",
    "'State Power Investment Corporation (SPIC)':'State Power Investment Corporation', \\\n",
    "'samruk-kazyna':'samruk energy', 'sarawak energy':'sarawak energy group',\\\n",
    "'shaanxi jitai investment group':'shaanxi investment group', \\\n",
    "'shaanxi non-ferrous metals holding group (shaanxi youser group)':'shaanxi youser group', \\\n",
    "'shaanxi non-ferrous metals holding group':'shaanxi youser group', \\\n",
    "'shandong xinhai tech co':'shandong xinhai technology', 'shenergy':'shenergy group', \\\n",
    "'shumba energy (former shumba coal)':'shumba energy', 'sichuan hongda':'sichuan hongda group', \\\n",
    "'sichuan provincial investment group':'sichuan investment group', 'sinar mas':'sinar mas group', \\\n",
    "'sks ispat & power limited':'sks ispat & power', \\\n",
    "'sunset power (delta electricity)':'delta electricity', 'sunset power international':'delta electricity', \\\n",
    "'Taiwan Power Company (Taipower)':'Taipower', 'Tamil Nadu Generation and Distribution Corp (Tangedco)': 'TANGEDCO', \\\n",
    "'Tamil Nadu Generation and Distribution Corporation (TANGEDCO)':'TANGEDCO', 'Tata Power':'Tata Group', \\\n",
    "'Tenaga Nasional Berhad (TNB)':'TNB', 'Tokyo Electric Power Company (TEPCO)':'TEPCO', \\\n",
    "'Toyo-Thai Corporation Public Company (TTCL)':'Toyo Engineering', \\\n",
    "'tauron group':'tauron polska energia', 'tbea (tebian electric apparatus)':'tbea', \\\n",
    "'telangana state power generation corp (tsgenco)':'tsgenco', 'texhong textile group':'texhong group',\\\n",
    "'Government of Uttar Pradesh':'thdc india', 'top frontier investment holdings':'san miguel corporation', \\\n",
    "'Uttar Pradesh Raijya Vidyut Utpadan Nigam (UPRVUN)':'UPRVUNL', \\\n",
    "'Vietnam Electricity Corporation (EVN)':'Vietnam Electricity (EVN)', 'EVN':'Vietnam Electricity (EVN)', \\\n",
    "'Vinacomin (Vietnam National Coal and Mineral Industries Group)':'Vinacomin', \\\n",
    "'vietnam oil and gas group (petrovietnam)':'petrovietnam',  'vivant corp.':'vivant corporation', \\\n",
    "'west bengal power development corporation limited (wbpdcl)':'wbpdcl', \\\n",
    "'wintime energy':'wintime holding group', \\\n",
    "'Yunnan Provincial Energy Investment Group':'Yunnan Energy investment Group', \\\n",
    "'yangquan coal industry':'yangquan coal industry group', \\\n",
    "'ytl power international':'ytl corporation', 'yunnan investment holdings group':'yunnan energy investment group', \\\n",
    "                 }\n",
    "\n",
    "dict_DEV_names_lowercase = dict((k.lower(), v.lower()) for k, v in dict_DEV_names.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVs in old dev : 2082\n",
      "DEVs in old fin : 204\n",
      "DEVs in old inv : 126\n",
      "DEVs in old dev lowercase: 2070\n",
      "DEVs in old fin lowercase: 202\n",
      "DEVs in old inv lowercase: 126\n"
     ]
    }
   ],
   "source": [
    "#number of DEVs before change:\n",
    "print('DEVs in old dev : '+str(len(set(df_dev_2020_01_final['DEV']))))\n",
    "print('DEVs in old fin : '+str(len(set(df_fin_2012_2019_final['DEV']))))\n",
    "print('DEVs in old inv : '+str(len(set(df_inv_2019_final['DEV']))))\n",
    "\n",
    "print('DEVs in old dev lowercase: '+str(len(set(df_dev_2020_01_final_names['DEV']))))\n",
    "print('DEVs in old fin lowercase: '+str(len(set(df_fin_2012_2019_final_names['DEV']))))\n",
    "print('DEVs in old inv lowercase: '+str(len(set(df_inv_2019_final_names['DEV']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change DEV names in all datasets! \n",
    "def change_DEV_names(DEV_name_old):\n",
    "    name = DEV_name_old\n",
    "    for new_name in dict_DEV_names_lowercase.keys():\n",
    "        if DEV_name_old ==  new_name:\n",
    "            name = dict_DEV_names_lowercase[new_name]\n",
    "    return(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change to new names:\n",
    "for x in df_dev_2020_01_final_names.index:\n",
    "    df_dev_2020_01_final_names.loc[x, 'DEV'] = change_DEV_names(df_dev_2020_01_final_names.loc[x, 'DEV'])\n",
    "    \n",
    "for x in df_fin_2012_2019_final_names.index:\n",
    "    df_fin_2012_2019_final_names.loc[x, 'DEV'] = change_DEV_names(df_fin_2012_2019_final_names.loc[x, 'DEV'])\n",
    "    \n",
    "for x in df_inv_2019_final_names.index:\n",
    "    df_inv_2019_final_names.loc[x, 'DEV'] = change_DEV_names(df_inv_2019_final_names.loc[x, 'DEV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVs in dev names: 2067\n",
      "DEVs in fin names: 173\n",
      "DEVs in inv names: 125\n"
     ]
    }
   ],
   "source": [
    "#check whether less DEV now:\n",
    "print('DEVs in dev names: '+str(len(set(df_dev_2020_01_final_names['DEV']))))\n",
    "print('DEVs in fin names: '+str(len(set(df_fin_2012_2019_final_names['DEV']))))\n",
    "print('DEVs in inv names: '+str(len(set(df_inv_2019_final_names['DEV']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a dataframe with all the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframes for fin & inv with DEVs, later add DEVs from dev\n",
    "#first column: all DEV from fin+inv\n",
    "set_fin_a_inv_DEV = set(df_fin_2012_2019_final_names['DEV'])|set(df_inv_2019_final_names['DEV'])\n",
    "\n",
    "df_DEVs = pd.DataFrame(data=list(set_fin_a_inv_DEV), columns=['fin_a_inv'])#list\n",
    "df_DEVs.sort_values(by=['fin_a_inv'],inplace=True)\n",
    "df_DEVs.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now add new column (second), with DEV from dev\n",
    "df_DEVs['dev_fin_a_inv'] = ''\n",
    "list_DEV_not_found_in_dev = []\n",
    "for x in df_DEVs.index:\n",
    "    DEV_name = df_DEVs.loc[x, 'fin_a_inv']\n",
    "    if DEV_name in set(df_dev_2020_01_final_names['DEV']):\n",
    "        df_DEVs.loc[x, 'dev_fin_a_inv'] = DEV_name\n",
    "    else:\n",
    "        list_DEV_not_found_in_dev.append(df_DEVs.loc[x, 'fin_a_inv'])\n",
    "len(list_DEV_not_found_in_dev)\n",
    "\n",
    "df_DEVs.to_csv('.//for_paper//df_overview//df_DEVs_fin_2012_2019_inv_2019_dev_2020_01.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_2020_01_final_names.to_csv('.//for_paper//df_overview//df_dev_'+z+'_final_names.csv', \\\n",
    "                                  decimal=',', sep=';')\n",
    "df_fin_2012_2019_final_names.to_csv('.//for_paper//df_overview//df_fin_2012_2019_final_names.csv', \\\n",
    "                                    decimal=',', sep=';')\n",
    "df_inv_2019_final_names.to_csv('.//for_paper//df_overview//df_inv_2019_final_names.csv', \\\n",
    "                               decimal=',', sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
